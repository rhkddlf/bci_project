{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><font size=10>Preprocessing</font><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-segmentation\" data-toc-modified-id=\"Data-segmentation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data segmentation</a></span></li><li><span><a href=\"#Spliting-training-and-test-data\" data-toc-modified-id=\"Spliting-training-and-test-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Spliting training and test data</a></span></li><li><span><a href=\"#Normalization\" data-toc-modified-id=\"Normalization-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Normalization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Z-score-normalization\" data-toc-modified-id=\"Z-score-normalization-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Z-score normalization</a></span></li><li><span><a href=\"#Uniform-normalization-(L1)\" data-toc-modified-id=\"Uniform-normalization-(L1)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Uniform normalization (L1)</a></span></li><li><span><a href=\"#Min-max-normalization\" data-toc-modified-id=\"Min-max-normalization-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Min-max normalization</a></span></li></ul></li><li><span><a href=\"#Reference\" data-toc-modified-id=\"Reference-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Reference</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data segmentation\n",
    "\n",
    "Next, we show how to segment the data stream. We take 0.1 second as an example of time window, therefore, each segment contains 16=0.6 $\\times$ 160 time steps as the sampling rate is 160 Hz. We set the overlapping rate as 50%, indicating there are 50% overlap between two adjacent segments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After segmentation, the shape of the data: (64, 225)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import myimporter\n",
    "from BCI_functions import *  # BCI_functions.ipynb contains some functions we might use multiple times in this tutorial\n",
    "\n",
    "dataset_1=np.load('1.npy')  # load data\n",
    "\n",
    "n_class = 2  # 0~9 classes ('10:rest' is not considered)\n",
    "no_feature = 14  # the number of the features\n",
    "segment_length = 16  # selected time window; 16=160*0.1\n",
    "\n",
    "# segment data, check more details about the 'extract' function in BCI_functions.ipynb\n",
    "data_seg = extract(dataset_1, n_classes=n_class, n_fea=no_feature, time_window=segment_length, moving=(segment_length/2))  # 50% overlapping\n",
    "\n",
    "print('After segmentation, the shape of the data:', data_seg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting training and test data\n",
    "\n",
    "In this part, we will show how to spliting the dataset into training data and test data. Just simply use the well-packed function from sklearn: https://scikit-learn.org/stable/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of training feature and label: (48, 225) (48, 0) \n",
      "The shape of test feature and label: (16, 225) (16, 0)\n"
     ]
    }
   ],
   "source": [
    "# First, we separate the features and the label \n",
    "\n",
    "data_seg_feature = data_seg[:, :1024]\n",
    "data_seg_label = data_seg[:, 1024:1025]\n",
    "\n",
    "# split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_feature, test_feature, train_label, test_label = train_test_split(data_seg_feature, data_seg_label, shuffle=True)\n",
    "print('The shape of training feature and label:', train_feature.shape, train_label.shape,\n",
    "     '\\nThe shape of test feature and label:', test_feature.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "In this section, we introduce a number of commonly used normalization strategies including z-score, uniform, and min-max normalization.  \n",
    "The library we adopt in this part is scikit-learn, if you are new to this, please check more details from: https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "* **Why we need normalization?**  \n",
    " It's important for some machine learning methods that all features are on the same scale (e.g., faster convergence in learning, more uniform influence for all weights), which may enable the model to achieve better results.\n",
    " \n",
    "* **Time-wise normalization vs. channel-wise normalization**  \n",
    " The time-wise normalization is to rescale data along the time-wise while the channel-wise is to rescale along the channel-wise to a consistent distribution. Normally, for EEG signals, we only focus on the time-wise normalization because of its high temporal resolution.     \n",
    " <font color=#FF0000> Take our data X of shape (n_samples, n_channels) as an example, time-wise normalization means scaling along the sample axis. </font>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-score normalization\n",
    "\n",
    "Z-score normalization (Standardization) gives you an idea of how far from the mean a data point is.   \n",
    "The equation for z-score normalization:\n",
    "$$x_i^{'}=\\frac{x_{i}-\\bar{x_{i}}}{\\sigma }$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After normalization, the shape of training feature: (48, 225) \n",
      "After normalization, the shape of test feature: (16, 225)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "scaler1 = StandardScaler().fit(train_feature)\n",
    "train_fea_norm1 = scaler1.transform(train_feature) # normalize the training data\n",
    "test_fea_norm1 = scaler1.transform(test_feature) # normalize the test data\n",
    "\n",
    "print('After normalization, the shape of training feature:', train_fea_norm1.shape,\n",
    "      '\\nAfter normalization, the shape of test feature:', test_fea_norm1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform normalization (L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the uniform normalization, it process the dataset in a way that the sum of the absolute value of each column (in our case) to be one.   \n",
    "The equation for uniform normalization:\n",
    "$$x_i^{'}=\\frac{x_{i}}{\\sum x_{j}}$$\n",
    "\n",
    "* Note, in _sklearn.preprocessing.Normalizer_ï¼Œ the default normalization is L2, so remember to set the normalizer to L1 normalization (norm='l1') when you want to use uniform normalization.\n",
    "* Mind the axis in this one. The normalizer seems to be defaultly scale on the channel-wise axis, in this case, we have to transpose the data in order to get the correct normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 16 features, but Normalizer is expecting 48 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12984/897768498.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mscaler2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_feature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_fea_norm2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_feature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtest_fea_norm2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_feature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtrain_fea_norm2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fea_norm2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   2045\u001b[0m         \"\"\"\n\u001b[0;32m   2046\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2047\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2048\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ensure_2d'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    366\u001b[0m                 \u001b[1;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m                 f\"is expecting {self.n_features_in_} features as input.\")\n",
      "\u001b[1;31mValueError\u001b[0m: X has 16 features, but Normalizer is expecting 48 features as input."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "scaler2 = Normalizer(norm='l1').fit(train_feature.T)  \n",
    "train_fea_norm2 = scaler2.transform(train_feature.T) \n",
    "test_fea_norm2 = scaler2.transform(test_feature.T) \n",
    "\n",
    "train_fea_norm2 = train_fea_norm2.T\n",
    "test_fea_norm2 = test_fea_norm2.T\n",
    "\n",
    "print('After normalization, the shape of training feature:', train_fea_norm2.shape,\n",
    "      '\\nAfter normalization, the shape of test feature:', test_fea_norm2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-max normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-max normalization is one of the most common ways to normalize data. For every feature, the minimum value of that feature gets transformed into a 0, the maximum value gets transformed into a 1, and every other value gets transformed into a decimal between 0 and 1.[<sup>1</sup>](#refer-anchor-1)   \n",
    "The equation for min-max normalization:  \n",
    "$$x_i^{'}=\\frac{x_{i}-x_{min}}{x_{max}-x_{min}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After normalization, the shape of training feature: (48, 225) \n",
      "After normalization, the shape of test feature: (16, 225)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler3 = MinMaxScaler().fit(train_feature)\n",
    "train_fea_norm3 = scaler3.transform(train_feature)\n",
    "test_fea_norm3 = scaler3.transform(test_feature)\n",
    "\n",
    "print('After normalization, the shape of training feature:', train_fea_norm3.shape,\n",
    "      '\\nAfter normalization, the shape of test feature:', test_fea_norm3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "<div id=\"refer-anchor-1\"></div>\n",
    "\n",
    "- [1]  [Normalization](https://www.codecademy.com/articles/normalization#:~:text=Min%2Dmax%20normalization%20is%20one,decimal%20between%200%20and%201.&text=That%20data%20is%20just%20as%20squished%20as%20before!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
